{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06b62e08",
   "metadata": {},
   "source": [
    "### Notice\n",
    "Everything should be ran from the main directory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5f2067",
   "metadata": {},
   "source": [
    "You need to install all required dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971983c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abadca89",
   "metadata": {},
   "source": [
    "# Training \n",
    "Train the custom nanoGPT model with every tokenizer, for 1000 steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee371ef",
   "metadata": {},
   "source": [
    "We used the Wikitext.txt file for training each tokenizer, but you can use as well the TaylorSwiftWiki.txt (Taylor swift Wiki page) or the tiny_shakespeare.txt (sample from Shakespeare's book)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2211503f",
   "metadata": {},
   "source": [
    "### Byte level\n",
    "Should take 1-2 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf73caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run utils/train_nanogpt_mod.py --dataset data/Wikitext.txt --tokenizer byt5 --max_steps 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca5cb86",
   "metadata": {},
   "source": [
    "### Character level\n",
    "Should take 1-2 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d67a897",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run utils/train_nanogpt_mod.py --dataset data/Wikitext.txt --tokenizer char --max_steps 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bcaa1e0",
   "metadata": {},
   "source": [
    "### Byte-Pair Encoding\n",
    "Should take 6-7 hours with a strong GPU. We recommend not to do that and just use the pre-generated checkpoint file. Here you can define your desired vocabulary size by changing --vocab_size value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2f7c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run utils/train_nanogpt_mod.py --dataset data/Wikitext.txt --tokenizer bpe --vocab_size 32000 --max_steps 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76281246",
   "metadata": {},
   "source": [
    "# Evaluating\n",
    "Evaluate the best checkpoints that the model generated for each tokenization method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3edc85b",
   "metadata": {},
   "source": [
    "### Byte level\n",
    "Should take 1-2 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca1a4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run utils/enhanced_eval.py --checkpoint checkpoints/WikiText/nanogpt_byt5_step1000.pt --dataset data/Wikitext.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6d6ade",
   "metadata": {},
   "source": [
    "### Character level\n",
    "Should take 1-2 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ca2f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run utils/enhanced_eval.py --checkpoint checkpoints/WikiText/nanogpt_char_step1000.pt --dataset data/Wikitext.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e09791",
   "metadata": {},
   "source": [
    "### Byte-Pair Encoding\n",
    "Should take 6-7 hours with a strong GPU. We recommend not to do that and just use the pre-generated evaluation file (json)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0c9499",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run utils/enhanced_eval.py --checkpoint checkpoints/WikiText/nanogpt_bpe_step1000.pt --dataset data/Wikitext.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf8935c",
   "metadata": {},
   "source": [
    "At this point, metrics files have been generated for each tokenization method, inside results folder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6c3f18",
   "metadata": {},
   "source": [
    "# Comparison\n",
    "Now you can compare the tokenizers and generate some plots!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8728e1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run utils/compare_tokenizers.py results/enhanced_metrics_byt5_step1000.json results/enhanced_metrics_char_step1000.json results/enhanced_metrics_bpe_step1000.json --out_dir results/plots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b691982c",
   "metadata": {},
   "source": [
    "# Text Generation\n",
    "Also, you can try and generate some text using any prompt and any checkpoint!\n",
    "\n",
    "*Works only with the byte-level tokenizer right now :("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1afbc88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run utils/generate_nanogpt_mod.py --checkpoint checkpoints/WikiText/nanogpt_byt5_step1000.pt --prompt \"Hello\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepLearning_GPU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
